{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "714a0b51-c151-4938-9523-5f6431fe6396",
   "metadata": {},
   "source": [
    "# Prepare spike trains for gpfa and train models\n",
    "\n",
    "## !! This script generates gpfa_dict.pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "11919bb3-236d-4c73-878a-2ff14d07c70c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # IMPORTS\n",
    "# %matplotlib inline\n",
    "# %run -i '/home/jovyan/pablo_tostado/bird_song/manifold_paper_analysis/all_imports.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "87e3eeea-47de-476f-8329-fc197c2ac1b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "import numpy as np\n",
    "import quantities as pq\n",
    "\n",
    "from songbirdcore.statespace_analysis.gpfa_songbirdcore import GPFACore\n",
    "from songbirdcore.statespace_analysis.pca_songbirdcore import PCACore\n",
    "from songbirdcore.statespace_analysis.statespace_analysis_utils import convert_to_neo_spike_trains, convert_to_neo_spike_trains_3d\n",
    "\n",
    "import songbirdcore.spikefinder.spike_analysis_helper as sh\n",
    "from songbirdcore.utils.params import GlobalParams as gparams\n",
    "from songbirdcore.utils.data_utils import save_dataset\n",
    "\n",
    "np.random.seed(24)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hairy-princeton",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1a46d2a1-2666-48de-8373-52fb1fc045d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Dictionary!\n",
      "dict_keys(['neural_dict', 'audio_motifs', 'audio_labels', 'fs_neural', 'fs_audio', 't_pre', 't_post', 'sess_params'])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.1, 0.6)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # b1, RAW\n",
    "file_path = '/home/jovyan/pablo_tostado/bird_song/enSongDec/data/RAW_z_w12m7_20_20240325_210721.pkl'\n",
    "# b2, RAW\n",
    "# file_path = '/home/jovyan/pablo_tostado/bird_song/enSongDec/data/RAW_z_r12r13_21_20240328_185716.pkl'\n",
    "\n",
    "with open(file_path, 'rb') as pickle_file:\n",
    "    state_space_analysis_dict = pkl.load(pickle_file)\n",
    "\n",
    "print(\"Loaded Dictionary!\")\n",
    "\n",
    "print(state_space_analysis_dict.keys())\n",
    "\n",
    "neural_dict = state_space_analysis_dict['neural_dict']\n",
    "audio_motifs = state_space_analysis_dict['audio_motifs']\n",
    "audio_labels = state_space_analysis_dict['audio_labels']\n",
    "fs_neural = state_space_analysis_dict['fs_neural']\n",
    "fs_audio = state_space_analysis_dict['fs_audio']\n",
    "t_pre = state_space_analysis_dict['t_pre']\n",
    "t_post = state_space_analysis_dict['t_post']\n",
    "sess_params = state_space_analysis_dict['sess_params']\n",
    "\n",
    "t_pre, t_post"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60582bf8-e29d-4d43-80cd-34d7c76e6f2d",
   "metadata": {},
   "source": [
    "### Drop silent clusters (at least 1 spike per trial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aeae411b-70cb-4728-bc29-ea9040260779",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ra_sua (10, 93, 21000)\n",
      "ra_all (10, 172, 21000)\n",
      "hvc_sua (10, 46, 21000)\n",
      "hvc_all (10, 114, 21000)\n"
     ]
    }
   ],
   "source": [
    "for k in neural_dict.keys():\n",
    "    num_trials = len(neural_dict[k])\n",
    "    neural_dict[k] = np.delete(neural_dict[k], np.where(np.sum(neural_dict[k], axis=(0,2))<num_trials)[0], axis=1)\n",
    "    \n",
    "    print(k, neural_dict[k].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "538c0cfd-c2dc-4a53-a6af-287f777b189d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Generate spiketrains for shuffle control conditions\n",
    "\n",
    "# neural_groups = list(neural_dict.keys())\n",
    "\n",
    "# for k in neural_groups:\n",
    "#     neural_dict[k+'_shuffle_time'] = np.array([permute_array_rows_independently(i) for i in neural_dict[k]])\n",
    "#     neural_dict[k+'_shuffle_neurons'] = np.array([permute_array_cols_independently(i) for i in neural_dict[k]])\n",
    "\n",
    "# display(neural_dict.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90647c99-1810-46de-8b92-6a413820ed59",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Fit GPFA & PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e2b6af05-a99c-43ce-81a0-7cee24ab3dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# State-Space Analysis Params\n",
    "bin_size = 5 * pq.ms\n",
    "latent_dim = [12] # neural_dict[key].shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16878e44-f61d-43c9-a08e-fb176abc90a6",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Loop to fit PCA and GPFA to each neural group of interest (including controls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4a0c8441-300a-4d61-9b93-1974da9ea92a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ra_all\n",
      "subsampling 50.0% of the neural channels.\n",
      "ra_all original_neural_traces (10, 86, 21000)\n",
      "Initializing parameters using factor analysis...\n",
      "\n",
      "Fitting GPFA model...\n",
      "ra_all complimentary_neural_traces (10, 86, 21000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-05-09 05:12:46,557] utils - WARNING: Correcting 1 rounding errors by shifting the affected spikes into the following bin. You can set tolerance=None to disable this behaviour.\n",
      "[2024-05-09 05:12:46,573] utils - WARNING: Correcting 1 rounding errors by shifting the affected spikes into the following bin. You can set tolerance=None to disable this behaviour.\n",
      "[2024-05-09 05:12:46,612] utils - WARNING: Correcting 1 rounding errors by shifting the affected spikes into the following bin. You can set tolerance=None to disable this behaviour.\n",
      "[2024-05-09 05:12:46,621] utils - WARNING: Correcting 1 rounding errors by shifting the affected spikes into the following bin. You can set tolerance=None to disable this behaviour.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing parameters using factor analysis...\n",
      "\n",
      "Fitting GPFA model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2024-05-09 05:16:26,247] utils - WARNING: Correcting 1 rounding errors by shifting the affected spikes into the following bin. You can set tolerance=None to disable this behaviour.\n",
      "[2024-05-09 05:16:26,265] utils - WARNING: Correcting 1 rounding errors by shifting the affected spikes into the following bin. You can set tolerance=None to disable this behaviour.\n",
      "[2024-05-09 05:16:26,329] utils - WARNING: Correcting 1 rounding errors by shifting the affected spikes into the following bin. You can set tolerance=None to disable this behaviour.\n",
      "[2024-05-09 05:16:26,337] utils - WARNING: Correcting 1 rounding errors by shifting the affected spikes into the following bin. You can set tolerance=None to disable this behaviour.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary saved as /home/jovyan/pablo_tostado/bird_song/enSongDec/data/RAW_z_w12m7_20_20240509_051627_ra_all_latent_stability_dataset.pkl\n",
      "Dictionary saved as /home/jovyan/pablo_tostado/bird_song/enSongDec/data/TRAJECTORIES_z_w12m7_20_20240509_051630_ra_all_latent_stability_dataset.pkl\n"
     ]
    }
   ],
   "source": [
    "# neural_groups = list(neural_dict.keys())\n",
    "neural_groups = ['ra_all']\n",
    "latent_models = ['pca', 'gpfa']\n",
    "neural_samp_perc = 0.5\n",
    "\n",
    "# Initialize dictionaries to store PCA & GPFA state-space analysis results for each neural group\n",
    "neural_splits = ['original_neural_traces', 'complimentary_neural_traces']\n",
    "state_space_analysis_dict_raw = {ct: {} for ct in neural_splits}\n",
    "state_space_analysis_dict_trajectories = {ct: {} for ct in neural_splits}\n",
    "\n",
    "\n",
    "for ng in neural_groups:\n",
    "    \n",
    "    neural_traces = neural_dict[ng]\n",
    "    print(f'Processing {ng}')\n",
    "\n",
    "    # Randomly sample neural channels\n",
    "    if neural_samp_perc < 1:\n",
    "        print(f'subsampling {neural_samp_perc*100}% of the neural channels.')\n",
    "        num_channels = neural_traces.shape[1]\n",
    "        num_to_sample = round(num_channels * neural_samp_perc)  \n",
    "\n",
    "        all_indices = np.arange(num_channels)\n",
    "        # Randomly select a subset of indices\n",
    "        sampled_indices = np.random.choice(all_indices, num_to_sample, replace=False)\n",
    "        # Find the complementary set of indices\n",
    "        complementary_indices = np.setdiff1d(all_indices, sampled_indices)\n",
    "\n",
    "        # Dict with original and complimentary neural sets\n",
    "        combined_neural_traces = {}\n",
    "        combined_neural_traces[neural_splits[0]] = neural_traces[:, sampled_indices, :]\n",
    "        combined_neural_traces[neural_splits[1]] = neural_traces[:, complementary_indices, :]\n",
    "\n",
    "    for ct in combined_neural_traces.keys():\n",
    "        \n",
    "        neural_traces = combined_neural_traces[ct]\n",
    "        print(ng, ct, neural_traces.shape)\n",
    "        \n",
    "        # Dictionary to store PCA & GPFA state-space analysis results for ng\n",
    "        trajectories_dict = {k:{} for k in latent_models}\n",
    "    \n",
    "        for ld in latent_dim:\n",
    "            \n",
    "            # If not enough clusters for desired number of latent dimensions\n",
    "            if ld > neural_traces.shape[1]:\n",
    "                print(f'ld: {ld}, clusters: {neural_traces.shape[1]}. ld > num_clusters: skipping state-space analysis.')\n",
    "                trajectories_dict['pca'][k] = None\n",
    "                trajectories_dict['gpfa'][k] = None\n",
    "                continue\n",
    "    \n",
    "            \"\"\" Fit PCA \"\"\"\n",
    "            # Instantiate PCA\n",
    "            myPCA = PCACore(neural_traces, round(fs_neural), audio_motifs, fs_audio, audio_labels, fs_audio)\n",
    "            myPCA.instantiate_pca(ld)\n",
    "    \n",
    "            # Downsample spiketrains\n",
    "            spike_trains = sh.downsample_list_3d(myPCA.neural_traces, number_bin_samples=int(bin_size/1000*fs_neural), mode='sum')\n",
    "    \n",
    "            # Fit PCA\n",
    "            pca_dict = myPCA.fit_transform_pca(spike_trains)\n",
    "            pca_dict['bin_w'] = bin_size\n",
    "    \n",
    "            k = ng+'_dim'+str(ld)\n",
    "            trajectories_dict['pca'][k] = pca_dict\n",
    "    \n",
    "            \"\"\" Fit GPFA \"\"\"\n",
    "            # Instantiate GPFA\n",
    "            myGPFA = GPFACore(neural_traces, round(fs_neural), audio_motifs, fs_audio, audio_labels, fs_audio)\n",
    "    \n",
    "            # Run GPFA in spiketrains of targer neural data\n",
    "            myGPFA.instantiate_gpfa(bin_size, ld, em_max_iters=gparams.gpfa_max_iter);\n",
    "    \n",
    "            k = ng+'_dim'+str(ld)\n",
    "            spike_trains = convert_to_neo_spike_trains_3d(myGPFA.neural_traces, myGPFA.fs_neural)\n",
    "            trajectories_dict['gpfa'][k] = myGPFA.fit_transform_gpfa(spike_trains);\n",
    "        \n",
    "        state_space_analysis_dict_raw[ct][ng] = neural_traces\n",
    "        state_space_analysis_dict_trajectories[ct][ng] = trajectories_dict\n",
    "\n",
    "\n",
    "# Save params\n",
    "dir_path = '/home/jovyan/pablo_tostado/bird_song/enSongDec/data/'\n",
    "filename_appendix = f\"{ng}_latent_stability_dataset\"\n",
    "# Save RAW\n",
    "file_type = 'RAW'\n",
    "raw_fs_neural = fs_neural\n",
    "save_dataset(\n",
    "    state_space_analysis_dict_raw,\n",
    "    audio_motifs,\n",
    "    audio_labels,\n",
    "    raw_fs_neural,\n",
    "    fs_audio,\n",
    "    t_pre,\n",
    "    t_post,\n",
    "    sess_params, \n",
    "    file_type,\n",
    "    dir_path, \n",
    "    filename_appendix)\n",
    "\n",
    "# Save trajectories\n",
    "file_type = 'TRAJECTORIES'\n",
    "traj_fs_neural = 1/int(bin_size)*1000\n",
    "save_dataset(\n",
    "    state_space_analysis_dict_trajectories,\n",
    "    audio_motifs,\n",
    "    audio_labels,\n",
    "    traj_fs_neural,\n",
    "    fs_audio,\n",
    "    t_pre,\n",
    "    t_post,\n",
    "    sess_params, \n",
    "    file_type,\n",
    "    dir_path, \n",
    "    filename_appendix)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "630db7b8-5418-4b2b-81bd-fc2158b27628",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_dict_tree(d, indent=0):\n",
    "    for key, value in d.items():\n",
    "        print('  ' * indent + str(key))\n",
    "        if isinstance(value, dict):\n",
    "            print_dict_tree(value, indent + 1)\n",
    "\n",
    "\n",
    "# print_dict_tree(state_space_analysis_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2007da3-0163-48ba-b8c3-07117f8a6206",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
